{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83b24837",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Auditing an MLM trained on WikiText using an LRT Attack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ed55c1",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "In this tutorial, we will see:\n",
    "\n",
    "- How to specify the dataset and model for Privacy Meter\n",
    "- How to audit a Tensorflow model\n",
    "- How to use the `ReferenceMetric` to evaluate membership leakage using loss values from reference models\n",
    "- How to visualize the audit result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73dfc8ee",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e571ad5b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ee7e3da",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /Users/aadyaamaddi/miniconda3/envs/mlpmenv/lib/python3.6/site-packages (1.13.3)\n",
      "Requirement already satisfied: dill in /Users/aadyaamaddi/miniconda3/envs/mlpmenv/lib/python3.6/site-packages (from datasets) (0.3.4)\n",
      "Requirement already satisfied: pandas in /Users/aadyaamaddi/miniconda3/envs/mlpmenv/lib/python3.6/site-packages (from datasets) (1.1.5)\n",
      "Requirement already satisfied: xxhash in /Users/aadyaamaddi/miniconda3/envs/mlpmenv/lib/python3.6/site-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: importlib-metadata in /Users/aadyaamaddi/miniconda3/envs/mlpmenv/lib/python3.6/site-packages (from datasets) (4.8.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /Users/aadyaamaddi/miniconda3/envs/mlpmenv/lib/python3.6/site-packages (from datasets) (4.63.0)\n",
      "Requirement already satisfied: multiprocess in /Users/aadyaamaddi/miniconda3/envs/mlpmenv/lib/python3.6/site-packages (from datasets) (0.70.12.2)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /Users/aadyaamaddi/miniconda3/envs/mlpmenv/lib/python3.6/site-packages (from datasets) (2022.2.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/aadyaamaddi/miniconda3/envs/mlpmenv/lib/python3.6/site-packages (from datasets) (2.27.1)\n",
      "Requirement already satisfied: aiohttp in /Users/aadyaamaddi/miniconda3/envs/mlpmenv/lib/python3.6/site-packages (from datasets) (3.7.4.post0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/aadyaamaddi/miniconda3/envs/mlpmenv/lib/python3.6/site-packages (from datasets) (1.19.5)\n",
      "Requirement already satisfied: huggingface-hub<0.1.0,>=0.0.19 in /Users/aadyaamaddi/miniconda3/envs/mlpmenv/lib/python3.6/site-packages (from datasets) (0.0.19)\n",
      "Requirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /Users/aadyaamaddi/miniconda3/envs/mlpmenv/lib/python3.6/site-packages (from datasets) (3.0.0)\n",
      "Requirement already satisfied: packaging in /Users/aadyaamaddi/miniconda3/envs/mlpmenv/lib/python3.6/site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: dataclasses in /Users/aadyaamaddi/miniconda3/envs/mlpmenv/lib/python3.6/site-packages (from datasets) (0.8)\n",
      "Requirement already satisfied: filelock in /Users/aadyaamaddi/miniconda3/envs/mlpmenv/lib/python3.6/site-packages (from huggingface-hub<0.1.0,>=0.0.19->datasets) (3.4.0)\n",
      "Requirement already satisfied: typing-extensions in /Users/aadyaamaddi/miniconda3/envs/mlpmenv/lib/python3.6/site-packages (from huggingface-hub<0.1.0,>=0.0.19->datasets) (4.1.1)\n",
      "Requirement already satisfied: pyyaml in /Users/aadyaamaddi/miniconda3/envs/mlpmenv/lib/python3.6/site-packages (from huggingface-hub<0.1.0,>=0.0.19->datasets) (5.4.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/aadyaamaddi/miniconda3/envs/mlpmenv/lib/python3.6/site-packages (from packaging->datasets) (3.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/aadyaamaddi/miniconda3/envs/mlpmenv/lib/python3.6/site-packages (from requests>=2.19.0->datasets) (2020.6.20)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/aadyaamaddi/miniconda3/envs/mlpmenv/lib/python3.6/site-packages (from requests>=2.19.0->datasets) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/aadyaamaddi/miniconda3/envs/mlpmenv/lib/python3.6/site-packages (from requests>=2.19.0->datasets) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/aadyaamaddi/miniconda3/envs/mlpmenv/lib/python3.6/site-packages (from requests>=2.19.0->datasets) (1.26.8)\n",
      "Requirement already satisfied: importlib-resources in /Users/aadyaamaddi/miniconda3/envs/mlpmenv/lib/python3.6/site-packages (from tqdm>=4.62.1->datasets) (5.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/aadyaamaddi/miniconda3/envs/mlpmenv/lib/python3.6/site-packages (from aiohttp->datasets) (21.4.0)\n",
      "Requirement already satisfied: chardet<5.0,>=2.0 in /Users/aadyaamaddi/miniconda3/envs/mlpmenv/lib/python3.6/site-packages (from aiohttp->datasets) (4.0.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/aadyaamaddi/miniconda3/envs/mlpmenv/lib/python3.6/site-packages (from aiohttp->datasets) (1.6.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/aadyaamaddi/miniconda3/envs/mlpmenv/lib/python3.6/site-packages (from aiohttp->datasets) (5.2.0)\n",
      "Requirement already satisfied: idna-ssl>=1.0 in /Users/aadyaamaddi/miniconda3/envs/mlpmenv/lib/python3.6/site-packages (from aiohttp->datasets) (1.1.0)\n",
      "Requirement already satisfied: async-timeout<4.0,>=3.0 in /Users/aadyaamaddi/miniconda3/envs/mlpmenv/lib/python3.6/site-packages (from aiohttp->datasets) (3.0.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/aadyaamaddi/miniconda3/envs/mlpmenv/lib/python3.6/site-packages (from importlib-metadata->datasets) (3.6.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Users/aadyaamaddi/miniconda3/envs/mlpmenv/lib/python3.6/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in /Users/aadyaamaddi/miniconda3/envs/mlpmenv/lib/python3.6/site-packages (from pandas->datasets) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/aadyaamaddi/miniconda3/envs/mlpmenv/lib/python3.6/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: transformers in /Users/aadyaamaddi/miniconda3/envs/mlpmenv/lib/python3.6/site-packages (4.11.3)\n",
      "Requirement already satisfied: filelock in /Users/aadyaamaddi/miniconda3/envs/mlpmenv/lib/python3.6/site-packages (from transformers) (3.4.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.0.17 in /Users/aadyaamaddi/miniconda3/envs/mlpmenv/lib/python3.6/site-packages (from transformers) (0.0.19)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/aadyaamaddi/miniconda3/envs/mlpmenv/lib/python3.6/site-packages (from transformers) (1.19.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/aadyaamaddi/miniconda3/envs/mlpmenv/lib/python3.6/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/aadyaamaddi/miniconda3/envs/mlpmenv/lib/python3.6/site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/aadyaamaddi/miniconda3/envs/mlpmenv/lib/python3.6/site-packages (from transformers) (2022.3.15)\n",
      "Requirement already satisfied: requests in /Users/aadyaamaddi/miniconda3/envs/mlpmenv/lib/python3.6/site-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: sacremoses in /Users/aadyaamaddi/miniconda3/envs/mlpmenv/lib/python3.6/site-packages/sacremoses-0.0.43-py3.8.egg (from transformers) (0.0.43)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /Users/aadyaamaddi/miniconda3/envs/mlpmenv/lib/python3.6/site-packages (from transformers) (0.10.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/aadyaamaddi/miniconda3/envs/mlpmenv/lib/python3.6/site-packages (from transformers) (4.63.0)\n",
      "Requirement already satisfied: dataclasses in /Users/aadyaamaddi/miniconda3/envs/mlpmenv/lib/python3.6/site-packages (from transformers) (0.8)\n",
      "Requirement already satisfied: importlib_metadata in /Users/aadyaamaddi/miniconda3/envs/mlpmenv/lib/python3.6/site-packages (from transformers) (4.8.3)\n",
      "Requirement already satisfied: typing-extensions in /Users/aadyaamaddi/miniconda3/envs/mlpmenv/lib/python3.6/site-packages (from huggingface-hub>=0.0.17->transformers) (4.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/aadyaamaddi/miniconda3/envs/mlpmenv/lib/python3.6/site-packages (from packaging>=20.0->transformers) (3.0.7)\n",
      "Requirement already satisfied: importlib-resources in /Users/aadyaamaddi/miniconda3/envs/mlpmenv/lib/python3.6/site-packages (from tqdm>=4.27->transformers) (5.4.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/aadyaamaddi/miniconda3/envs/mlpmenv/lib/python3.6/site-packages (from importlib_metadata->transformers) (3.6.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/aadyaamaddi/miniconda3/envs/mlpmenv/lib/python3.6/site-packages (from requests->transformers) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/aadyaamaddi/miniconda3/envs/mlpmenv/lib/python3.6/site-packages (from requests->transformers) (1.26.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/aadyaamaddi/miniconda3/envs/mlpmenv/lib/python3.6/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/aadyaamaddi/miniconda3/envs/mlpmenv/lib/python3.6/site-packages (from requests->transformers) (2020.6.20)\n",
      "Requirement already satisfied: six in /Users/aadyaamaddi/miniconda3/envs/mlpmenv/lib/python3.6/site-packages (from sacremoses->transformers) (1.16.0)\n",
      "Requirement already satisfied: click in /Users/aadyaamaddi/miniconda3/envs/mlpmenv/lib/python3.6/site-packages (from sacremoses->transformers) (8.0.3)\n",
      "Requirement already satisfied: joblib in /Users/aadyaamaddi/miniconda3/envs/mlpmenv/lib/python3.6/site-packages (from sacremoses->transformers) (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m pip install datasets\n",
    "!{sys.executable} -m pip install transformers\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0c1b127a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///Users/aadyaamaddi/Desktop/ML%20Privacy%20Meter/privacy_meter\n",
      "Installing collected packages: privacy-meter\n",
      "  Attempting uninstall: privacy-meter\n",
      "    Found existing installation: privacy-meter 1.0\n",
      "    Uninstalling privacy-meter-1.0:\n",
      "      Successfully uninstalled privacy-meter-1.0\n",
      "  Running setup.py develop for privacy-meter\n",
      "Successfully installed privacy-meter-1.0\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m pip install -e ../.\n",
    "from privacy_meter.audit import Audit\n",
    "from privacy_meter.audit_report import ROCCurveReport, SignalHistogramReport\n",
    "from privacy_meter.constants import *\n",
    "from privacy_meter.dataset import Dataset\n",
    "from privacy_meter.hypothesis_test import linear_itp_threshold_func\n",
    "from privacy_meter.information_source import InformationSource\n",
    "from privacy_meter.information_source_signal import Signal, ModelMaskedLoss\n",
    "from privacy_meter.metric import Metric\n",
    "from privacy_meter.metric_result import MetricResult\n",
    "from privacy_meter.model import HuggingFaceMaskedLanguageModel\n",
    "from privacy_meter.utils import flatten_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7802f1e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d12b56",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Setting seed for reproducibility:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f6f914c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "seed = 1234\n",
    "np.random.seed(seed)\n",
    "rng = np.random.default_rng(seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ee72a9",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c787a5d0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# for the target and reference models\n",
    "num_train_data = 2500\n",
    "num_test_data = 2500\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "prob_mask = 0.15\n",
    "num_times_mask = 20\n",
    "stride = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90266375",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# for the reference metric\n",
    "num_reference_models = 1\n",
    "fpr_tolerance_list = [\n",
    "    0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81af2c28",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Load dataset and models using HuggingFace "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51a98144",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset wikitext (/Users/aadyaamaddi/.cache/huggingface/datasets/wikitext/wikitext-103-v1/1.0.0/aa5e094000ec7afeb74c3be92c88313cd6f132d564c7effd961c10fd47c76f20)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53a5bccefa954d03b3b29664266fcafe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "hf_dataset = load_dataset('wikitext', 'wikitext-103-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38271157",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "target_tokenizer = AutoTokenizer.from_pretrained(\"saghar/TinyBERT_General_6L_768D-finetuned-wikitext103\")\n",
    "target_model = AutoModelForMaskedLM.from_pretrained(\"saghar/TinyBERT_General_6L_768D-finetuned-wikitext103\")\n",
    "\n",
    "reference_tokenizer = AutoTokenizer.from_pretrained(\"saghar/MiniLMv2-L6-H384-distilled-from-RoBERTa-Large-finetuned-wikitext103\")\n",
    "reference_model = AutoModelForMaskedLM.from_pretrained(\"saghar/MiniLMv2-L6-H384-distilled-from-RoBERTa-Large-finetuned-wikitext103\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e9843c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/aadyaamaddi/.cache/huggingface/datasets/wikitext/wikitext-103-v1/1.0.0/aa5e094000ec7afeb74c3be92c88313cd6f132d564c7effd961c10fd47c76f20/cache-645845fc37c81c46.arrow\n",
      "Loading cached processed dataset at /Users/aadyaamaddi/.cache/huggingface/datasets/wikitext/wikitext-103-v1/1.0.0/aa5e094000ec7afeb74c3be92c88313cd6f132d564c7effd961c10fd47c76f20/cache-25033f423669cef0.arrow\n",
      "Loading cached processed dataset at /Users/aadyaamaddi/.cache/huggingface/datasets/wikitext/wikitext-103-v1/1.0.0/aa5e094000ec7afeb74c3be92c88313cd6f132d564c7effd961c10fd47c76f20/cache-e9ed7429da72da98.arrow\n"
     ]
    }
   ],
   "source": [
    "# remove empty rows from dataset\n",
    "hf_dataset = hf_dataset.filter(lambda elem: len(elem[\"text\"]) > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7db5f323",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/aadyaamaddi/.cache/huggingface/datasets/wikitext/wikitext-103-v1/1.0.0/aa5e094000ec7afeb74c3be92c88313cd6f132d564c7effd961c10fd47c76f20/cache-b007a0c9695aa736.arrow\n",
      "Loading cached processed dataset at /Users/aadyaamaddi/.cache/huggingface/datasets/wikitext/wikitext-103-v1/1.0.0/aa5e094000ec7afeb74c3be92c88313cd6f132d564c7effd961c10fd47c76f20/cache-5b3f8589dfded72e.arrow\n",
      "Loading cached processed dataset at /Users/aadyaamaddi/.cache/huggingface/datasets/wikitext/wikitext-103-v1/1.0.0/aa5e094000ec7afeb74c3be92c88313cd6f132d564c7effd961c10fd47c76f20/cache-01167fbc392876fb.arrow\n",
      "Loading cached processed dataset at /Users/aadyaamaddi/.cache/huggingface/datasets/wikitext/wikitext-103-v1/1.0.0/aa5e094000ec7afeb74c3be92c88313cd6f132d564c7effd961c10fd47c76f20/cache-1390f8526f0a161e.arrow\n",
      "Loading cached processed dataset at /Users/aadyaamaddi/.cache/huggingface/datasets/wikitext/wikitext-103-v1/1.0.0/aa5e094000ec7afeb74c3be92c88313cd6f132d564c7effd961c10fd47c76f20/cache-ee27a5ea0fd545a4.arrow\n",
      "Loading cached processed dataset at /Users/aadyaamaddi/.cache/huggingface/datasets/wikitext/wikitext-103-v1/1.0.0/aa5e094000ec7afeb74c3be92c88313cd6f132d564c7effd961c10fd47c76f20/cache-8d6c3322ed7c88c5.arrow\n",
      "Loading cached processed dataset at /Users/aadyaamaddi/.cache/huggingface/datasets/wikitext/wikitext-103-v1/1.0.0/aa5e094000ec7afeb74c3be92c88313cd6f132d564c7effd961c10fd47c76f20/cache-e273ac3789b9db25.arrow\n",
      "Loading cached processed dataset at /Users/aadyaamaddi/.cache/huggingface/datasets/wikitext/wikitext-103-v1/1.0.0/aa5e094000ec7afeb74c3be92c88313cd6f132d564c7effd961c10fd47c76f20/cache-afd1e266d14b3a3e.arrow\n",
      "Loading cached processed dataset at /Users/aadyaamaddi/.cache/huggingface/datasets/wikitext/wikitext-103-v1/1.0.0/aa5e094000ec7afeb74c3be92c88313cd6f132d564c7effd961c10fd47c76f20/cache-5a1a625e861d5013.arrow\n",
      "Loading cached processed dataset at /Users/aadyaamaddi/.cache/huggingface/datasets/wikitext/wikitext-103-v1/1.0.0/aa5e094000ec7afeb74c3be92c88313cd6f132d564c7effd961c10fd47c76f20/cache-1100dbd6e0513036.arrow\n",
      "Loading cached processed dataset at /Users/aadyaamaddi/.cache/huggingface/datasets/wikitext/wikitext-103-v1/1.0.0/aa5e094000ec7afeb74c3be92c88313cd6f132d564c7effd961c10fd47c76f20/cache-fd2828d1b2f9a28b.arrow\n",
      "Loading cached processed dataset at /Users/aadyaamaddi/.cache/huggingface/datasets/wikitext/wikitext-103-v1/1.0.0/aa5e094000ec7afeb74c3be92c88313cd6f132d564c7effd961c10fd47c76f20/cache-2e131506bbc2af4c.arrow\n"
     ]
    }
   ],
   "source": [
    "# tokenize function for target model\n",
    "def target_tokenize_fn(examples):\n",
    "    return target_tokenizer(examples[\"text\"])\n",
    "\n",
    "# tokenize function for reference model\n",
    "def reference_tokenize_fn(examples):\n",
    "    return reference_tokenizer(examples[\"text\"])\n",
    "\n",
    "tokenized_target_hf_dataset = hf_dataset.map(\n",
    "    target_tokenize_fn, batched=True, num_proc=2, remove_columns=[\"text\"]\n",
    ")\n",
    "\n",
    "tokenized_reference_hf_dataset = hf_dataset.map(\n",
    "    reference_tokenize_fn, batched=True, num_proc=2, remove_columns=[\"text\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c81dde72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/aadyaamaddi/.cache/huggingface/datasets/wikitext/wikitext-103-v1/1.0.0/aa5e094000ec7afeb74c3be92c88313cd6f132d564c7effd961c10fd47c76f20/cache-671e99d3d11e214d.arrow\n",
      "Loading cached processed dataset at /Users/aadyaamaddi/.cache/huggingface/datasets/wikitext/wikitext-103-v1/1.0.0/aa5e094000ec7afeb74c3be92c88313cd6f132d564c7effd961c10fd47c76f20/cache-cf1c982b41bbb521.arrow\n",
      "Loading cached processed dataset at /Users/aadyaamaddi/.cache/huggingface/datasets/wikitext/wikitext-103-v1/1.0.0/aa5e094000ec7afeb74c3be92c88313cd6f132d564c7effd961c10fd47c76f20/cache-d48b87d84e81d8fd.arrow\n",
      "Loading cached processed dataset at /Users/aadyaamaddi/.cache/huggingface/datasets/wikitext/wikitext-103-v1/1.0.0/aa5e094000ec7afeb74c3be92c88313cd6f132d564c7effd961c10fd47c76f20/cache-35243fbdcd6b24fb.arrow\n",
      "Loading cached processed dataset at /Users/aadyaamaddi/.cache/huggingface/datasets/wikitext/wikitext-103-v1/1.0.0/aa5e094000ec7afeb74c3be92c88313cd6f132d564c7effd961c10fd47c76f20/cache-e67687d5de84a56e.arrow\n",
      "Loading cached processed dataset at /Users/aadyaamaddi/.cache/huggingface/datasets/wikitext/wikitext-103-v1/1.0.0/aa5e094000ec7afeb74c3be92c88313cd6f132d564c7effd961c10fd47c76f20/cache-4206bb41c4b2e1ba.arrow\n",
      "Loading cached processed dataset at /Users/aadyaamaddi/.cache/huggingface/datasets/wikitext/wikitext-103-v1/1.0.0/aa5e094000ec7afeb74c3be92c88313cd6f132d564c7effd961c10fd47c76f20/cache-710a7806d707c03b.arrow\n",
      "Loading cached processed dataset at /Users/aadyaamaddi/.cache/huggingface/datasets/wikitext/wikitext-103-v1/1.0.0/aa5e094000ec7afeb74c3be92c88313cd6f132d564c7effd961c10fd47c76f20/cache-7eebd6ff043c2620.arrow\n",
      "Loading cached processed dataset at /Users/aadyaamaddi/.cache/huggingface/datasets/wikitext/wikitext-103-v1/1.0.0/aa5e094000ec7afeb74c3be92c88313cd6f132d564c7effd961c10fd47c76f20/cache-1c76be21d92d15b6.arrow\n",
      "Loading cached processed dataset at /Users/aadyaamaddi/.cache/huggingface/datasets/wikitext/wikitext-103-v1/1.0.0/aa5e094000ec7afeb74c3be92c88313cd6f132d564c7effd961c10fd47c76f20/cache-816ac7072640ccd5.arrow\n",
      "Loading cached processed dataset at /Users/aadyaamaddi/.cache/huggingface/datasets/wikitext/wikitext-103-v1/1.0.0/aa5e094000ec7afeb74c3be92c88313cd6f132d564c7effd961c10fd47c76f20/cache-d75f52a4ed5f6dfc.arrow\n",
      "Loading cached processed dataset at /Users/aadyaamaddi/.cache/huggingface/datasets/wikitext/wikitext-103-v1/1.0.0/aa5e094000ec7afeb74c3be92c88313cd6f132d564c7effd961c10fd47c76f20/cache-53fbfff0d120deed.arrow\n"
     ]
    }
   ],
   "source": [
    "# batch dataset\n",
    "block_size = 128\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    \n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    \n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    \n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "lm_target_hf_dataset = tokenized_target_hf_dataset.map(\n",
    "    group_texts, batched=True, batch_size=64, num_proc=2\n",
    ")\n",
    "\n",
    "lm_reference_hf_dataset = tokenized_reference_hf_dataset.map(\n",
    "    group_texts, batched=True, batch_size=64, num_proc=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b834918",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# split into train and test for target model\n",
    "target_train_split = lm_target_hf_dataset[\"train\"].select(range(num_train_data))\n",
    "target_test_split =  lm_target_hf_dataset[\"test\"].select(range(num_test_data))\n",
    "\n",
    "# split into train and test for reference model\n",
    "# note: we create new objects for the reference model because it can use a different tokenizer,\n",
    "#       but the original text for the sequences will be the same as they are for the target model\n",
    "reference_train_split = lm_reference_hf_dataset[\"train\"].select(range(num_train_data))\n",
    "reference_test_split =  lm_reference_hf_dataset[\"test\"].select(range(num_test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "746adb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set format of datasets to be compatible with Privacy Meter\n",
    "target_train_split.set_format(\"numpy\")\n",
    "target_test_split.set_format(\"numpy\")\n",
    "reference_train_split.set_format(\"numpy\")\n",
    "reference_test_split.set_format(\"numpy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e686ae81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Dataset for target model\n",
    "train_ds = {'input': target_train_split['input_ids']}\n",
    "test_ds = {'input': target_test_split['input_ids']}\n",
    "target_dataset = Dataset(\n",
    "    data_dict={'train': train_ds, 'test': test_ds},\n",
    "    default_input='input',\n",
    "    default_output=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5145d589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Dataset for reference model\n",
    "train_ds = {'input': reference_train_split['input_ids']}\n",
    "test_ds = {'input': reference_test_split['input_ids']}\n",
    "reference_dataset = Dataset(\n",
    "    data_dict={'train': train_ds, 'test': test_ds},\n",
    "    default_input='input',\n",
    "    default_output=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a499abd",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# create Privacy Meter compatible wrapper objects for the models\n",
    "wrapped_target_model = HuggingFaceMaskedLanguageModel(model_obj=target_model, loss_fn=loss_fn,\n",
    "                                                      tokenizer=target_tokenizer,\n",
    "                                                      prob_mask=prob_mask,\n",
    "                                                      num_times_mask=num_times_mask,\n",
    "                                                      stride=stride)\n",
    "wrapped_reference_model = HuggingFaceMaskedLanguageModel(model_obj=target_model, loss_fn=loss_fn,\n",
    "                                                         tokenizer=reference_tokenizer,\n",
    "                                                         prob_mask=prob_mask, \n",
    "                                                         num_times_mask=num_times_mask,\n",
    "                                                         stride=stride)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2cf473",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Information Sources\n",
    "\n",
    "We can now define two `InformationSource` objects. Basically, an information source is an abstraction representing a set of models, and their corresponding dataset. Note that for the `ReferenceMetric` we use the same dataset in both the target and reference information sources, but the models that will be used for querying the dataset will differ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "15ffcbae",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "target_info_source = InformationSource(\n",
    "    models=[wrapped_target_model],\n",
    "    datasets=[target_dataset]\n",
    ")\n",
    "\n",
    "reference_info_source = InformationSource(\n",
    "    models=[wrapped_reference_model],\n",
    "    datasets=[reference_dataset]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b83dd40",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Metric and Audit\n",
    "\n",
    "We now create a `Metric` object, which is an abstraction representing an algorithm used to measure something on an `InformationSource`, such as membership information leakage. In this case, we use the `ReferenceMetric` to measure the membership information leakage of `target_info_source` in a black-box setting, using loss values returned by the reference model on the target dataset in `reference_info_source`.\n",
    "\n",
    "The `Audit` object is a wrapper to actually run the audit, and display the results. More visualization options will be added soon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "67ac3792",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Optional, List, Tuple, Union\n",
    "\n",
    "class LrtMetric(Metric):\n",
    "    \"\"\"\n",
    "    Inherits from the Metric class to perform the LRT membership inference attack which will be used as a metric\n",
    "    for measuring privacy leakage of a target model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            target_info_source: InformationSource,\n",
    "            reference_info_source: InformationSource,\n",
    "            signals: List[Signal],\n",
    "            hypothesis_test_func: Optional[Callable],\n",
    "            target_model_to_train_split_mapping: List[Tuple[int, str, str, str]] = None,\n",
    "            target_model_to_test_split_mapping: List[Tuple[int, str, str, str]] = None,\n",
    "            reference_model_to_train_split_mapping: List[Tuple[int, str, str, str]] = None,\n",
    "            reference_model_to_test_split_mapping: List[Tuple[int, str, str, str]] = None,\n",
    "            unique_dataset: bool = False,\n",
    "            logs_dirname: str = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Constructor\n",
    "\n",
    "        Args:\n",
    "            target_info_source: InformationSource, containing the Model that the metric will be performed on, and the\n",
    "                corresponding Dataset.\n",
    "            reference_info_source: List of InformationSource(s), containing the Model(s) that the metric will be\n",
    "                fitted on, and their corresponding Dataset.\n",
    "            signals: List of signals to be used.\n",
    "            hypothesis_test_func: Function that will be used for computing attack threshold(s)\n",
    "            target_model_to_train_split_mapping: Mapping from the target model to the train split of the target dataset.\n",
    "                By default, the code will look for a split named \"train\"\n",
    "            target_model_to_test_split_mapping: Mapping from the target model to the test split of the target dataset.\n",
    "                By default, the code will look for a split named \"test\"\n",
    "            reference_model_to_train_split_mapping: Mapping from the reference models to their train splits of the\n",
    "                corresponding reference dataset. By default, the code will look for a split named \"train\"\n",
    "            reference_model_to_test_split_mapping: Mapping from the reference models to their test splits of the\n",
    "                corresponding reference dataset. By default, the code will look for a split named \"test\"\n",
    "            unique_dataset: Boolean indicating if target_info_source and target_info_source use one same dataset object.\n",
    "        \"\"\"\n",
    "\n",
    "        # Initializes the parent metric\n",
    "        super().__init__(target_info_source=target_info_source,\n",
    "                         reference_info_source=reference_info_source,\n",
    "                         signals=signals,\n",
    "                         hypothesis_test_func=hypothesis_test_func,\n",
    "                         logs_dirname=logs_dirname)\n",
    "\n",
    "        # Logs directory\n",
    "        self.logs_dirname = logs_dirname\n",
    "\n",
    "        # Store the model to split mappings\n",
    "        self.target_model_to_train_split_mapping = target_model_to_train_split_mapping\n",
    "        self.target_model_to_test_split_mapping = target_model_to_test_split_mapping\n",
    "\n",
    "        # Custom default mapping for the reference metric\n",
    "        if reference_model_to_train_split_mapping is None:\n",
    "            self.reference_model_to_train_split_mapping = [\n",
    "                                                              (0, 'train', '<default_input>', '<default_output>')\n",
    "                                                          ] * len(self.reference_info_source.models)\n",
    "        if reference_model_to_test_split_mapping is None:\n",
    "            self.reference_model_to_test_split_mapping = [\n",
    "                                                             (0, 'test', '<default_input>', '<default_output>')\n",
    "                                                         ] * len(self.reference_info_source.models)\n",
    "\n",
    "        self._set_default_mappings(unique_dataset)\n",
    "\n",
    "        # Variables used in prepare_metric and run_metric\n",
    "        self.member_signals, self.non_member_signals = [], []\n",
    "        self.reference_member_signals, self.reference_non_member_signals = [], []\n",
    "        self.pointwise_member_thresholds, self.pointwise_non_member_thresholds = [], []\n",
    "\n",
    "    def prepare_metric(self):\n",
    "        \"\"\"\n",
    "        Function to prepare data needed for running the metric on the target model and dataset, using signals computed\n",
    "        on the reference model(s) and dataset. For the reference attack, the reference models will be a list of models\n",
    "        trained on data from the same distribution, and the reference dataset will be the target model's train-test\n",
    "        split.\n",
    "        \"\"\"\n",
    "        # Load signals if they have been computed already; otherwise, compute and save them\n",
    "        self.member_signals = flatten_array(self._load_or_compute_signal(SignalSourceEnum.TARGET_MEMBER))\n",
    "        self.non_member_signals = flatten_array(self._load_or_compute_signal(SignalSourceEnum.TARGET_NON_MEMBER))\n",
    "        self.reference_member_signals = np.array(\n",
    "            self._load_or_compute_signal(SignalSourceEnum.REFERENCE_MEMBER)[0]).transpose()\n",
    "        self.reference_non_member_signals = np.array(\n",
    "            self._load_or_compute_signal(SignalSourceEnum.REFERENCE_NON_MEMBER)[0]).transpose()\n",
    "\n",
    "    def run_metric(self, fpr_tolerance_rate_list=None) -> List[MetricResult]:\n",
    "        \"\"\"\n",
    "        Function to run the metric on the target model and dataset.\n",
    "\n",
    "        Args:\n",
    "            fpr_tolerance_rate_list (optional): List of FPR tolerance values that may be used by the threshold function\n",
    "                to compute the attack threshold for the metric.\n",
    "\n",
    "        Returns:\n",
    "            A list of MetricResult objects, one per fpr value.\n",
    "        \"\"\"\n",
    "        lrt_signals = self.target_non_member_signals - self.reference_non_member_signals\n",
    "            \n",
    "        metric_result_list = []\n",
    "        for fpr_tolerance_rate in fpr_tolerance_rate_list:   \n",
    "            # Use global threshold\n",
    "            threshold = self.hypothesis_test_func(lrt_signals, fpr_tolerance_rate)\n",
    "                \n",
    "            member_lrt_values, member_preds = [], []\n",
    "            for idx, signal in enumerate(self.member_signals):\n",
    "                lrt_value = self.signal - self.reference_member_signals[idx]\n",
    "                if lrt_value <= threshold:\n",
    "                    member_preds.append(1)\n",
    "                else:\n",
    "                    member_preds.append(0)\n",
    "                member_lrt_values.append(lrt_value)\n",
    "\n",
    "            non_member_lrt_values, non_member_preds = [], []\n",
    "            for idx, signal in enumerate(self.non_member_signals):\n",
    "                lrt_value = self.signal - self.reference_non_member_signals[idx]\n",
    "                if lrt_value <= threshold:\n",
    "                    non_member_preds.append(1)\n",
    "                else:\n",
    "                    non_member_preds.append(0)\n",
    "                non_member_lrt_values.append(lrt_value)\n",
    "\n",
    "            predictions = np.concatenate([member_preds, non_member_preds])\n",
    "\n",
    "            true_labels = [1] * len(self.member_signals)\n",
    "            true_labels.extend([0] * len(self.non_member_signals))\n",
    "            \n",
    "            signal_values = np.concatenate([member_lrt_values, non_member_lrt_values])\n",
    "\n",
    "            metric_result = MetricResult(metric_id=MetricEnum.LRT.value,\n",
    "                                         predicted_labels=predictions,\n",
    "                                         true_labels=true_labels,\n",
    "                                         predictions_proba=None,\n",
    "                                         signal_values=signal_values)\n",
    "\n",
    "            metric_result_list.append(metric_result)\n",
    "\n",
    "        return metric_result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "da9464f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lrt_metric = LrtMetric(\n",
    "    target_info_source=target_info_source,\n",
    "    reference_info_source=reference_info_source,\n",
    "    signals=[ModelMaskedLoss()],\n",
    "    hypothesis_test_func=linear_itp_threshold_func,\n",
    "    logs_dirname='mlm_lrt_metric'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "68ee2a66",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BertConfig' object has no attribute 'n_positions'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-4e7f335c0416>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mfpr_tolerances\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfpr_tolerance_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m )\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0maudit_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/ML Privacy Meter/privacy_meter/privacy_meter/audit.py\u001b[0m in \u001b[0;36mprepare\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    140\u001b[0m         \"\"\"\n\u001b[1;32m    141\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetric_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetric_objects\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mMetricResult\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-15343b7d6ee3>\u001b[0m in \u001b[0;36mprepare_metric\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     80\u001b[0m         \"\"\"\n\u001b[1;32m     81\u001b[0m         \u001b[0;31m# Load signals if they have been computed already; otherwise, compute and save them\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmember_signals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflatten_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_or_compute_signal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSignalSourceEnum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTARGET_MEMBER\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnon_member_signals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflatten_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_or_compute_signal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSignalSourceEnum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTARGET_NON_MEMBER\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         self.reference_member_signals = np.array(\n",
      "\u001b[0;32m~/Desktop/ML Privacy Meter/privacy_meter/privacy_meter/metric.py\u001b[0m in \u001b[0;36m_load_or_compute_signal\u001b[0;34m(self, signal_source)\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0msignal\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignals\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m                 signals.append(\n\u001b[0;32m---> 92\u001b[0;31m                     \u001b[0minfo_source_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_signal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msignal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapping_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m                 )\n\u001b[1;32m     94\u001b[0m             \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavez\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msignal_filepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/ML Privacy Meter/privacy_meter/privacy_meter/information_source.py\u001b[0m in \u001b[0;36mget_signal\u001b[0;34m(self, signal, model_to_split_mapping, extra)\u001b[0m\n\u001b[1;32m     75\u001b[0m                       \u001b[0mdatasets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m                       \u001b[0mmodel_to_split_mapping\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_to_split_mapping\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m                       extra=extra)\n\u001b[0m",
      "\u001b[0;32m~/Desktop/ML Privacy Meter/privacy_meter/privacy_meter/information_source_signal.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, models, datasets, model_to_split_mapping, extra)\u001b[0m\n\u001b[1;32m    375\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_feature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m             \u001b[0;31m# Compute the signal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m             \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_masked_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/ML Privacy Meter/privacy_meter/privacy_meter/model.py\u001b[0m in \u001b[0;36mget_masked_loss\u001b[0;34m(self, batch_samples)\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0mA\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mmasked\u001b[0m \u001b[0mloss\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    586\u001b[0m         \"\"\"\n\u001b[0;32m--> 587\u001b[0;31m         \u001b[0mmax_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_positions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m         \u001b[0mCLS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"[CLS]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/mlpmenv/lib/python3.6/site-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36m__getattribute__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"attribute_map\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"attribute_map\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"attribute_map\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'BertConfig' object has no attribute 'n_positions'"
     ]
    }
   ],
   "source": [
    "audit_obj = Audit(\n",
    "    metrics=[lrt_metric],\n",
    "    inference_game_type=InferenceGame.PRIVACY_LOSS_MODEL,\n",
    "    target_info_sources=[target_info_source],\n",
    "    reference_info_sources=[reference_info_source],\n",
    "    fpr_tolerances=fpr_tolerance_list\n",
    ")\n",
    "audit_obj.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263c53b1",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "audit_results = audit_obj.run()[0]\n",
    "for result in audit_results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eec9d4a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Result visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d92efb",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Several visualization tools are built in `privacy_tool`, such as ROC curves, signal values histogram, or confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a513b2",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# This instruction won't be needed once the tool is on pip\n",
    "from privacy_meter import audit_report\n",
    "audit_report.REPORT_FILES_DIR = '../privacy_meter/report_files'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6326f16a",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ROCCurveReport.generate_report(\n",
    "    metric_result=audit_results,\n",
    "    inference_game_type=InferenceGame.PRIVACY_LOSS_MODEL,\n",
    "    show=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1573603e",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "SignalHistogramReport.generate_report(\n",
    "    metric_result=result,\n",
    "    inference_game_type=InferenceGame.PRIVACY_LOSS_MODEL,\n",
    "    show=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

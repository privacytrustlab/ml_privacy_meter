{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c33bce9",
   "metadata": {},
   "source": [
    "# Auditing a Causal Language Model (LM) using the Population Attack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b26ec80",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52763e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b234f1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!{sys.executable} -m pip install datasets\n",
    "!{sys.executable} -m pip install transformers\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03ae357b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///Users/aadyaamaddi/Desktop/ML%20Privacy%20Meter/privacy_meter\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hInstalling collected packages: privacy-meter\n",
      "  Attempting uninstall: privacy-meter\n",
      "    Found existing installation: privacy-meter 1.0\n",
      "    Uninstalling privacy-meter-1.0:\n",
      "      Successfully uninstalled privacy-meter-1.0\n",
      "  Running setup.py develop for privacy-meter\n",
      "Successfully installed privacy-meter-1.0\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m pip install -e ../.\n",
    "from privacy_meter.audit import Audit\n",
    "from privacy_meter.dataset import Dataset\n",
    "from privacy_meter.hypothesis_test import threshold_func\n",
    "from privacy_meter.information_source import InformationSource\n",
    "from privacy_meter.information_source_signal import Signal\n",
    "from privacy_meter.model import Model\n",
    "from privacy_meter.metric import PopulationMetric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984f8e98",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2cd43118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a toy example for the population metric\n",
    "num_train_seqs = 25\n",
    "num_test_seqs = 25\n",
    "num_population_seqs = 100\n",
    "fpr_tolerance_list = [0.0, 0.1, 0.5, 0.9, 1.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f608226",
   "metadata": {},
   "source": [
    "## Load dataset and model using HuggingFace "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ee6e0d",
   "metadata": {},
   "source": [
    "### Load and split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6a25579",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset wikitext (/Users/aadyaamaddi/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/aa5e094000ec7afeb74c3be92c88313cd6f132d564c7effd961c10fd47c76f20)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caa6629df7594ae28102fb87133c0f08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/aadyaamaddi/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/aa5e094000ec7afeb74c3be92c88313cd6f132d564c7effd961c10fd47c76f20/cache-609f0466edcbe563.arrow\n",
      "Loading cached processed dataset at /Users/aadyaamaddi/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/aa5e094000ec7afeb74c3be92c88313cd6f132d564c7effd961c10fd47c76f20/cache-a5b4e128a9044f3b.arrow\n",
      "Loading cached processed dataset at /Users/aadyaamaddi/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/aa5e094000ec7afeb74c3be92c88313cd6f132d564c7effd961c10fd47c76f20/cache-1791161d42f2a24c.arrow\n"
     ]
    }
   ],
   "source": [
    "# download dataset from huggingface\n",
    "hf_dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "\n",
    "# remove empty rows\n",
    "hf_dataset = hf_dataset.filter(lambda elem: len(elem[\"text\"]) > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9593e25d",
   "metadata": {},
   "source": [
    "### Load and finetune model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7942897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pretrained and tokenizer\n",
    "model_id = \"distilgpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "model_obj = AutoModelForCausalLM.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3379ea0",
   "metadata": {},
   "source": [
    "We tokenize and group the dataset before finetuning the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57247626",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/aadyaamaddi/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/aa5e094000ec7afeb74c3be92c88313cd6f132d564c7effd961c10fd47c76f20/cache-07a2b89aceb1d793.arrow\n",
      "Loading cached processed dataset at /Users/aadyaamaddi/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/aa5e094000ec7afeb74c3be92c88313cd6f132d564c7effd961c10fd47c76f20/cache-56af95e21015d1ce.arrow\n",
      "Loading cached processed dataset at /Users/aadyaamaddi/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/aa5e094000ec7afeb74c3be92c88313cd6f132d564c7effd961c10fd47c76f20/cache-d120a339b23b77ea.arrow\n",
      "Loading cached processed dataset at /Users/aadyaamaddi/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/aa5e094000ec7afeb74c3be92c88313cd6f132d564c7effd961c10fd47c76f20/cache-125bb6af6a49553b.arrow\n",
      "Loading cached processed dataset at /Users/aadyaamaddi/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/aa5e094000ec7afeb74c3be92c88313cd6f132d564c7effd961c10fd47c76f20/cache-8b3c8172ff555876.arrow\n",
      "Loading cached processed dataset at /Users/aadyaamaddi/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/aa5e094000ec7afeb74c3be92c88313cd6f132d564c7effd961c10fd47c76f20/cache-137ae12a91e7f444.arrow\n"
     ]
    }
   ],
   "source": [
    "def tokenize_fn(examples):\n",
    "    return tokenizer(examples[\"text\"])\n",
    "\n",
    "tokenized_hf_dataset = hf_dataset.map(\n",
    "    tokenize_fn, batched=True, num_proc=2, remove_columns=[\"text\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "814dfdea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/aadyaamaddi/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/aa5e094000ec7afeb74c3be92c88313cd6f132d564c7effd961c10fd47c76f20/cache-9b20bf2b2fbf2c34.arrow\n",
      "Loading cached processed dataset at /Users/aadyaamaddi/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/aa5e094000ec7afeb74c3be92c88313cd6f132d564c7effd961c10fd47c76f20/cache-c76424446d71afc4.arrow\n",
      "Loading cached processed dataset at /Users/aadyaamaddi/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/aa5e094000ec7afeb74c3be92c88313cd6f132d564c7effd961c10fd47c76f20/cache-0119225805f1ed09.arrow\n",
      "Loading cached processed dataset at /Users/aadyaamaddi/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/aa5e094000ec7afeb74c3be92c88313cd6f132d564c7effd961c10fd47c76f20/cache-7671af262a452651.arrow\n",
      "Loading cached processed dataset at /Users/aadyaamaddi/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/aa5e094000ec7afeb74c3be92c88313cd6f132d564c7effd961c10fd47c76f20/cache-f9d0b833b775e8ce.arrow\n",
      "Loading cached processed dataset at /Users/aadyaamaddi/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/aa5e094000ec7afeb74c3be92c88313cd6f132d564c7effd961c10fd47c76f20/cache-a872619cae0789f7.arrow\n"
     ]
    }
   ],
   "source": [
    "block_size = 128\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    \n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    \n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    \n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "lm_hf_dataset = tokenized_hf_dataset.map(\n",
    "    group_texts, batched=True, batch_size=64, num_proc=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34e017f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_args = TrainingArguments(num_train_epochs=5, output_dir=\"privacy_meter_logs/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb784f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 25\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 20\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20/20 02:47, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=20, training_loss=3.950325775146484, metrics={'train_runtime': 177.7039, 'train_samples_per_second': 0.703, 'train_steps_per_second': 0.113, 'total_flos': 4082761728000.0, 'train_loss': 3.950325775146484, 'epoch': 5.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_trainer = Trainer(\n",
    "    model=model_obj,\n",
    "    args=finetune_args,\n",
    "    train_dataset=lm_hf_dataset[\"train\"].select(range(num_train_seqs)),\n",
    "    eval_dataset=lm_hf_dataset[\"validation\"]\n",
    ")\n",
    "\n",
    "hf_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0123f22d",
   "metadata": {},
   "source": [
    "## Extend Privacy Meter to work with HuggingFace Causal LMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6549eb28",
   "metadata": {},
   "source": [
    "### Extend the Model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20f61e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HfCausalLMModel(Model):\n",
    "    \"\"\"Inherits of the Model class, an interface to query a model without any assumption on how it is implemented.\n",
    "    This particular class is to be used with HuggingFace Causal LM models.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_obj, loss_fn, stride=64):\n",
    "        \"\"\"Constructor\n",
    "        Args:\n",
    "            model_obj: model object\n",
    "            loss_fn: loss function\n",
    "            stride: window size that will be used by the fixed length \n",
    "            causal model for processing an input sequence\n",
    "        \"\"\"\n",
    "\n",
    "        # Initializes the parent model\n",
    "        super().__init__(model_obj, loss_fn)\n",
    "        \n",
    "        self.stride = stride\n",
    "        \n",
    "    def get_outputs(self, batch_samples):\n",
    "        \"\"\"Function to get the model output from a given input.\n",
    "        Args:\n",
    "            batch_samples: Model input\n",
    "        Returns:\n",
    "            Model output\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def get_loss(self, batch_samples, batch_labels):\n",
    "        \"\"\"Function to get the model loss on a given input and an expected output.\n",
    "        Args:\n",
    "            batch_samples: Model input\n",
    "            batch_labels: Model expected output\n",
    "        Returns:\n",
    "            The loss value, as defined by the loss_fn attribute.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def get_grad(self, batch_samples, batch_labels):\n",
    "        \"\"\"Function to get the gradient of the model loss with respect to the model parameters, on a given input and an\n",
    "        expected output.\n",
    "        Args:\n",
    "            batch_samples: Model input\n",
    "            batch_labels: Model expected output\n",
    "        Returns:\n",
    "            A list of gradients of the model loss (one item per layer) with respect to the model parameters.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def get_intermediate_outputs(self, layers, batch_samples, forward_pass=True):\n",
    "        \"\"\"Function to get the intermediate output of layers (a.k.a. features), on a given input.\n",
    "        Args:\n",
    "            layers: List of integers and/or strings, indicating which layers values should be returned\n",
    "            batch_samples: Model input\n",
    "            forward_pass: Boolean indicating if a new forward pass should be executed. If True, then a forward pass is\n",
    "                executed on batch_samples. Else, the result is the one of the last forward pass.\n",
    "        Returns:\n",
    "            A list of intermediate outputs of layers.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def get_perplexity(self, batch_samples):\n",
    "        \"\"\"Function to get the perplexity of the model loss, on a given input sequence.\n",
    "        Args:\n",
    "            batch_samples: Model input\n",
    "        Returns:\n",
    "            A list of perplexity values.\n",
    "        \"\"\" \n",
    "        max_length = self.model_obj.config.n_positions\n",
    "        \n",
    "        ppl_values = []\n",
    "        for sample in batch_samples:\n",
    "            sample_length = len(sample)\n",
    "            \n",
    "            sample = np.expand_dims(sample, axis=0) # the model takes in a batch of sequences\n",
    "            sample = torch.tensor(sample, dtype=torch.long)\n",
    "            \n",
    "            nlls = []\n",
    "            for i in range(0, sample_length, self.stride):\n",
    "                begin_loc = max(i + self.stride - max_length, 0)\n",
    "                end_loc = min(i + self.stride, sample_length)\n",
    "                \n",
    "                trg_len = end_loc - i  # may be different from stride on last loop\n",
    "                \n",
    "                input_ids = sample[:, begin_loc:end_loc]\n",
    "                target_ids = input_ids.clone()\n",
    "                target_ids[:, :-trg_len] = -100\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    outputs = self.model_obj(input_ids, labels=target_ids)\n",
    "                    neg_log_likelihood = outputs[0] * trg_len\n",
    "\n",
    "                nlls.append(neg_log_likelihood)\n",
    "            ppl = torch.exp(torch.stack(nlls).sum() / end_loc)\n",
    "            \n",
    "            ppl_values.append(ppl)\n",
    "            \n",
    "        return ppl_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079c7b3d",
   "metadata": {},
   "source": [
    "### Create a new Signal for computing perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec923988",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelPerplexity(Signal):\n",
    "    \"\"\"\n",
    "    Inherits from the Signal class, used to represent any type of signal that can be obtained from a Model and/or a Dataset.\n",
    "    This particular class is used to get the perplexity of a model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self,\n",
    "                 models: List[Model],\n",
    "                 datasets: List[Dataset],\n",
    "                 model_to_split_mapping: List[Tuple[int, str, str, str]],\n",
    "                 extra: dict\n",
    "                 ):\n",
    "        \"\"\"Built-in call method.\n",
    "        Args:\n",
    "            models: List of models that can be queried.\n",
    "            datasets: List of datasets that can be queried.\n",
    "            model_to_split_mapping: List of tuples, indicating how each model should query the dataset.\n",
    "                More specifically, for model #i:\n",
    "                model_to_split_mapping[i][0] contains the index of the dataset in the list,\n",
    "                model_to_split_mapping[i][1] contains the name of the split,\n",
    "                model_to_split_mapping[i][2] contains the name of the input feature,\n",
    "                model_to_split_mapping[i][3] contains the name of the output feature.\n",
    "                This can also be provided once and for all at the instantiation of InformationSource, through the\n",
    "                default_model_to_split_mapping argument.\n",
    "            extra: Dictionary containing any additional parameter that should be passed to the signal object.\n",
    "        Returns:\n",
    "            The signal value.\n",
    "        \"\"\"\n",
    "\n",
    "        results = []\n",
    "        # Compute the signal for each model\n",
    "        for k, model in enumerate(models):\n",
    "            # Extract the features to be used\n",
    "            dataset_index, split_name, input_feature, output_feature = model_to_split_mapping[k]\n",
    "            x = datasets[dataset_index].get_feature(split_name, input_feature)\n",
    "            # Compute the signal\n",
    "            results.append(model.get_perplexity(x))\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f72ac31",
   "metadata": {},
   "source": [
    "### Audit LM using PopulationMetric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914d0619",
   "metadata": {},
   "source": [
    "We first create the target and reference datasets using the `Dataset` class. Since we already processed the dataset before for finetuning, we do not need to pass a preprocessing function while creating the `Dataset` objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9387580a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into 'train', 'test', and 'population'\n",
    "train_split = lm_hf_dataset[\"train\"].select(range(num_train_seqs))\n",
    "test_split =  lm_hf_dataset[\"test\"].select(range(num_test_seqs))\n",
    "population_split = lm_hf_dataset[\"train\"].select(\n",
    "    range(num_train_seqs, (num_train_seqs + num_population_seqs))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3d3181c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set format of datasets to be compatible with Privacy Meter\n",
    "train_split.set_format(\"numpy\")\n",
    "test_split.set_format(\"numpy\")\n",
    "population_split.set_format(\"numpy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e53ed773",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = {'input': train_split['input_ids']}\n",
    "test_ds = {'input': test_split['input_ids']}\n",
    "population_ds = {'input': population_split['input_ids']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "937ea27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dataset = Dataset(\n",
    "    data_dict={'train': train_ds, 'test': test_ds},\n",
    "    default_input='input',\n",
    "    default_output=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "87041f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_dataset = Dataset(\n",
    "    data_dict={'train': population_ds},\n",
    "    default_input='input',\n",
    "    default_output=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef087cb",
   "metadata": {},
   "source": [
    "Then, we wrap the model we finetuned into our custom `HfCausalLMModel` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ff830896",
   "metadata": {},
   "outputs": [],
   "source": [
    "stride = 512\n",
    "target_model = HfCausalLMModel(\n",
    "    model_obj=model_obj,\n",
    "    loss_fn=None,\n",
    "    stride=stride\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7698beec",
   "metadata": {},
   "source": [
    "Next, we create the target and reference `InformationSource` objects for the audit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "00e6519e",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_info_source = InformationSource(\n",
    "    models=[target_model], \n",
    "    datasets=[target_dataset]\n",
    ")\n",
    "\n",
    "reference_info_source = InformationSource(\n",
    "    models=[target_model],\n",
    "    datasets=[reference_dataset]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdeec3dc",
   "metadata": {},
   "source": [
    "And now we create a `PopulationMetric` object that uses perplexity as the signal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c42991b",
   "metadata": {},
   "outputs": [],
   "source": [
    "population_metric_obj = PopulationMetric(\n",
    "    target_info_source=target_info_source,\n",
    "    reference_info_source=reference_info_source,\n",
    "    signals=[ModelPerplexity()],\n",
    "    hypothesis_test_func=threshold_func,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642a88f6",
   "metadata": {},
   "source": [
    "Finally, we create the `Audit` object and run the audit on our finetuned model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7055a3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "audit_obj = Audit(\n",
    "    metric=population_metric_obj,\n",
    "    target_info_source=target_info_source,\n",
    "    reference_info_source=reference_info_source,\n",
    "    fpr_tolerance_list=fpr_tolerance_list\n",
    ")\n",
    "audit_obj.prepare()\n",
    "\n",
    "audit_results = audit_obj.run()\n",
    "for result in audit_results:\n",
    "    print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

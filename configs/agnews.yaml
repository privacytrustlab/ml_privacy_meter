run: # Configurations for a specific run
  random_seed: 12345 # Integer number of specifying random seed
  log_dir: demo_agnews # String for indicating where to save all the information, including models and computed signals. We can reuse the models saved in the same log_dir.
  time_log: True # Indicate whether to log the time for each step
  num_experiments: 1 # Number of total experiments

audit: # Configurations for auditing
  privacy_game: privacy_loss_model # Indicate the privacy game from privacy_loss_model
  algorithm: RMIA # String for indicating the membership inference attack. We currently support the RMIA introduced by Zarifzadeh et al. 2024(https://openreview.net/pdf?id=sT7UJh5CTc)) and the LOSS attacks
  num_ref_models: 1 # Number of reference models used to audit each target model
  device: cuda:0 # String for indicating the device we want to use for inferring signals and auditing models
  report_log: report_rmia # String that indicates the folder where we save the log and auditing report
  batch_size: 10 # Integer number for indicating batch size for evaluating models and inferring signals.
  data_size: 1000

train: # Configuration for training
  model_name: gpt2 # String for indicating the model type. We support CNN, wrn28-1, wrn28-2, wrn28-10, vgg16, mlp, gpt2 and speedyresnet (requires cuda). More model types can be added in model.py.
  tokenizer: gpt2 # String for indicating the tokenizer type. It can be any tokenizer or local checkpoint supported by the transformers library.
  device: cuda:0 # String for indicating the device we want to use for training models.
  batch_size: 8
  learning_rate: 0.00005
  weight_decay: 0.01
  epochs: 1
  optimizer: adamw_torch
#  peft: # configuration for peft
#    type: lora # Specify finetuning method
#    fan_in_fan_out: True
#    r: 16
#    target_modules: ["c_attn", "c_proj", "c_fc"]


data: # Configuration for data
  dataset: agnews # String indicates the name of the dataset. We support cifar10, cifar100, purchase100 and texas100 and agnews by default.
  data_dir: data
  tokenize: True
  tokenizer: gpt2 # String for indicating the tokenizer type. It can be any tokenizer or local checkpoint supported by the transformers library.